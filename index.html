
<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Pratul Srinivasan</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="img/icon.png">
  <title>Pratul Srinivasan</title>
  </head>

  <body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
    <td style="padding:0px">
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:2.5%;width:63%;vertical-align:middle">
        <p class="name" style="text-align: center;">
          <name>Pratul Srinivasan</name>
        </p>
        <p>
          I'm a research scientist at <a href="https://research.google/">Google DeepMind</a> in San Francisco, where I work on problems at the intersection of computer vision, computer graphics, and machine learning.
        </p>
        <p>
          I received my PhD from the <a href="https://eecs.berkeley.edu/">EECS Department</a> at UC Berkeley in December 2020, where I was advised by <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a> and <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>.

        </p>
        <p>
          During my PhD, I interned twice at <a href="https://research.google/">Google Research</a>: at Mountain View in 2017 (hosted by <a href="https://jonbarron.info/">Jon Barron</a> in <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy's</a> group) and at New York City in 2018 (hosted by <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>).
        </p>
        <p>
          I graduated from Duke University in 2014, where I worked with <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a> on research problems in medical computer vision.
        </p>
        <p style="text-align:center">
          <a href="mailto:pratul.srinivasan@gmail.com">Email</a> &nbsp/&nbsp
          <a href="pdf/cv.pdf">CV</a> &nbsp/&nbsp
	        <a href="https://scholar.google.com/citations?user=aYyDsZ0AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
          <a href="https://twitter.com/_pratul_">Twitter</a>
        </p>
        </td>
        <td style="padding:2.5%;width:40%;max-width:40%">
	         <a href="img/avatar.jpg">
        <img src="img/avatar_circle.png" width="250px"></a>
        </td>
      </tr>
    </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Research and Publications</h2>
          <p>
            * denotes equal contribution co-authorship

          </p>
        </td>
      </tr>
    </tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="illum_stop()" onmouseover="illum_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='illum_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/illum.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/illum.jpg' width="160">
        </div>
        <script type="text/javascript">
          function illum_start() {
            document.getElementById('illum_image').style.opacity = "1";
          }

          function illum_stop() {
            document.getElementById('illum_image').style.opacity = "0";
          }
          illum_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://illuminerf.github.io/">
			<span class="papertitle">IllumiNeRF: 3D Relighting without Inverse Rendering</span>
        </a>
        <br>
				<a href="https://xiaoming-zhao.com/">Xiaoming Zhao</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
        <a href="https://henzler.github.io/">Philipp Henzler</a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://illuminerf.github.io">project page</a>
        /
        <a href="https://arxiv.org/abs/2406.06527">arXiv</a>
        <p></p>
        <p>
				3D relighting by distilling samples from a 2D image relighting diffusion model into a latent-variable NeRF.
        </p>
      </td>
    </tr>

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="img/nuvo.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/nuvo.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://pratulsrinivasan.github.io/nuvo/">
          <span class="papertitle">Nuvo: Neural UV Mapping for Unruly 3D Representations</span>
        </a>
        <br>
        <strong>Pratul Srinivasan</strong>,
        <a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        /
        <a href="http://arxiv.org/abs/2312.05283">arXiv</a>
        <p></p>
        <p>
        Use neural fields to recover editable UV mappings for challenging geometry (e.g. NeRFs, marching cubes meshes, DreamFusion).
        </p>
      </td>
    </tr>

    <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/cat3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/cat3d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function cat3d_start() {
            document.getElementById('cat3d_image').style.opacity = "1";
          }

          function cat3d_stop() {
            document.getElementById('cat3d_image').style.opacity = "0";
          }
          cat3d_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://cat3d.github.io/">
			<span class="papertitle">CAT3D: Create Anything in 3D with Multi-View Diffusion Models</span>
        </a>
        <br>
				<a href="https://ruiqigao.github.io/">Ruiqi Gao</a>*,
        <a href="https://holynski.org/">Aleksander Holynski</a>*, 
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://github.com/ArthurBrussee">Arthur Brussee</a>, 
				<a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
        <strong>Pratul Srinivasan</strong>,
				<a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://poolio.github.io/">Ben Poole</a>*

        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://cat3d.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.10314">arXiv</a>
        <p></p>
        <p>
				A system built around diffusion and NeRF that does text-to-3D, image-to-3D, and few-view reconstruction, trains in 1 minute, and renders at 60FPS in a browser.
        </p>
      </td>
    </tr>


    <tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bog_image'><video  width=100% muted autoplay loop>
          <source src="img/bog.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/bog.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function bog_start() {
            document.getElementById('bog_image').style.opacity = "1";
          }

          function bog_stop() {
            document.getElementById('bog_image').style.opacity = "0";
          }
          bog_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://creiser.github.io/binary_opacity_grid/">
          <span class="papertitle">Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis</span>
        </a>
        <br>
				<a href="https://creiser.github.io/">Christian Reiser</a>,
				<a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
				<strong>Pratul Srinivasan</strong>,
				<a href="https://dorverbin.github.io/">Dor Verbin</a>,
				<a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
				<a href="https://jonbarron.info/">Jonathan T. Barron</a>,
				<a href="https://phogzone.com/">Peter Hedman</a>*,
				<a href="https://www.cvlibs.net/">Andreas Geiger</a>*		
        <br>
        <em>SIGGRAPH</em>, 2024
        <br>
        <a href="https://creiser.github.io/binary_opacity_grid/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=2TPUmGRg8bM">video</a>
        /
        <a href="https://arxiv.org/abs/2402.12377">arXiv</a>
        <p></p>
        <p>
        Applying anti-aliasing to a discrete opacity grid lets you render a hard representation into a soft image, and this enables highly-detailed mesh recovery.
        </p>
      </td>
    </tr>

    <tr onmouseout="eclipse_stop()" onmouseover="eclipse_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='eclipse_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/eclipse_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/eclipse_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function eclipse_start() {
            document.getElementById('eclipse_image').style.opacity = "1";
          }

          function eclipse_stop() {
            document.getElementById('eclipse_image').style.opacity = "0";
          }
          eclipse_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dorverbin.github.io/eclipse">
          <span class="papertitle">Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</span>
        </a>
        <br>
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://www.eecs.harvard.edu/~zickler/Main/HomePage">Todd Zickler</a>,
        <strong>Pratul Srinivasan</strong>
        <br>
        <em>CVPR</em>, 2024 <font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://dorverbin.github.io/eclipse">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=amQLGyza3EU">video</a>
        /
        <a href="https://arxiv.org/abs/2305.16321">arXiv</a>
        <p></p>
        <p>
        Shadows cast by unobserved occluders provide a high-frequency cue for recovering illumination and materials.
        </p>
      </td>
    </tr>

    <tr onmouseout="recon_stop()" onmouseover="recon_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/recon.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/recon.png' width="160">
        </div>
        <script type="text/javascript">
          function recon_start() {
            document.getElementById('recon_image').style.opacity = "1";
          }

          function recon_stop() {
            document.getElementById('recon_image').style.opacity = "0";
          }
          recon_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://reconfusion.github.io/">
			<span class="papertitle">ReconFusion: 3D Reconstruction with Diffusion Priors</span>
        </a>
        <br>
        <a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu*</a>,
		    <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
        <a href="https://scholar.google.com/citations?user=_pKKv2QAAAAJ&hl=en/">Daniel Watson</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
		    <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://poolio.github.io/">Ben Poole</a>,
        <a href="https://holynski.org/">Aleksander Holynski*</a>
        <br>
        <em>CVPR</em>, 2024
        <br>
        <a href="https://reconfusion.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2312.02981">arXiv</a>
        <p></p>
        <p>
        Using a multi-image diffusion model as a regularizer lets you recover high-quality radiance fields from just a handful of images.
        </p>
      </td>
    </tr>

    <tr onmouseout="genten_stop()" onmouseover="genten_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='genten_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/genten.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/genten.jpg' width="160">
        </div>
        <script type="text/javascript">
          function genten_start() {
            document.getElementById('genten_image').style.opacity = "1";
          }

          function genten_stop() {
            document.getElementById('genten_image').style.opacity = "0";
          }
          genten_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://powers-of-10.github.io/">
          <span class="papertitle">Generative Powers of Ten</span>
        </a>
        <br>
        <a href="https://scholar.google.ch/citations?user=v3NXpJAAAAAJ&hl=en">Xiaojuan Wang</a>,
        <a href="https://www.linkedin.com/in/jannekontkanen/">Janne Kontkanen</a>,
        <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
        <a href="https://www.smseitz.com/">Steve Seitz</a>,
        <a href="https://www.irakemelmacher.com/">Ira Kemelmacher</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://holynski.org/">Aleksander Holynski</a>
        <br>
        <em>CVPR</em>, 2024
        <br>
        <a href="https://powers-of-10.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2312.02149">arXiv</a>
        <p></p>
        <p>
        Use a text-to-image model to generate consistent content across drastically varying scales.
        </p>
      </td>
    </tr>

    <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/portalscrop.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/portalscrop.jpg' width="160">
        </div>
        <script type="text/javascript">
          function zipnerf_start() {
            document.getElementById('zipnerf_image').style.opacity = "1";
          }

          function zipnerf_stop() {
            document.getElementById('zipnerf_image').style.opacity = "0";
          }
          zipnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://jonbarron.info/zipnerf">
          <span class="papertitle">Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</span>
        </a>
        <br>
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://phogzone.com/">Peter Hedman</a>
        <br>
        <em>ICCV</em>, 2023   <font color="tomato"><strong>(Oral Presentation, Best Paper Finalist)</strong></font>
        <br>
        <a href="http://jonbarron.info/zipnerf">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
        /
        <a href="https://arxiv.org/abs/2304.06706">arXiv</a>
        <p></p>
        <p>
        Combining mip-NeRF 360 and Instant NGP lets us reconstruct huge scenes.
        </p>
      </td>
    </tr>

    <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="img/bakedsdf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='img/bakedsdf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://bakedsdf.github.io/">
            <span class="papertitle">BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis</span>
          </a>
          <br>
          <a href="https://lioryariv.github.io/">Lior Yariv*</a>,
          <a href="https://phogzone.com/">Peter Hedman*</a>,
          <a href="https://creiser.github.io/">Christian Reiser</a>,
          <a href="https://dorverbin.github.io/">Dor Verbin</a>,
          <strong>Pratul Srinivasan</strong>,
          <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
          <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>
          <br>
          <em>SIGGRAPH</em>, 2023
          <br>
          <a href="https://bakedsdf.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a>
          /
          <a href="https://arxiv.org/abs/2302.14859">arXiv</a>
          <p></p>
          <p>
          We use SDFs to bake a NeRF-like model into a high quality mesh and do real-time view synthesis.
          </p>
        </td>
      </tr>


      <tr onmouseout="merf_stop()" onmouseover="merf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='merf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="img/merf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='img/merf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function merf_start() {
              document.getElementById('merf_image').style.opacity = "1";
            }

            function merf_stop() {
              document.getElementById('merf_image').style.opacity = "0";
            }
            merf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://merf42.github.io/">
            <span class="papertitle">MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes</span>
          </a>
          <br>
          <a href="https://creiser.github.io/">Christian Reiser</a>,
          <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
          <a href="https://dorverbin.github.io/">Dor Verbin</a>,
          <strong>Pratul Srinivasan</strong>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://www.cvlibs.net/">Andreas Geiger</a>,
          <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>
          <br>
          <em>SIGGRAPH</em>, 2023
          <br>
          <a href="https://merf42.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=3EACM2JAcxc">video</a>
          /
          <a href="https://arxiv.org/abs/2302.12249">arXiv</a>
          <p></p>
          <p>
          We use volumetric rendering with a sparse 3D feature grid and 2D feature planes to do real-time view synthesis.
          </p>
        </td>
      </tr>

    <tr onmouseout="vq3d_stop()" onmouseover="vq3d_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='vq3d_image'>
          <img src='img/vq3d.gif' width="160">
          </div>
          <img src='img/vq3d.png' width="160">
        </div>
        <script type="text/javascript">
          function vq3d_start() {
            document.getElementById('vq3d_image').style.opacity = "1";
          }

          function vq3d_stop() {
            document.getElementById('vq3d_image').style.opacity = "0";
          }
          vq3d_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://kylesargent.github.io/vq3d">
          <span class="papertitle">VQ3D: Learning a 3D Generative Model on ImageNet</span>
        </a>
        <br>
        <a href="https://kylesargent.github.io/">Kyle Sargent</a>,
        <a href="https://jykoh.com/">Jing Yu Koh</a>,
        <a href="https://sites.google.com/corp/view/hanzhang">Han Zhang</a>,
        <a href="https://scholar.google.com/citations?user=eZQNcvcAAAAJ&hl=en">Huiwen Chang</a>,
        <a href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ&hl=en">Charles Herrmann</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://jiajunwu.com/">Jiajun Wu</a>,
        <a href="https://deqings.github.io/">Deqing Sun</a>
        <br>
  <em>CVPR</em>, 2023 <font color="tomato"><strong>(Oral Presentation, Best Paper Finalist)</strong></font>
        <br>
        <a href="https://kylesargent.github.io/vq3d">project page</a>
        /
        <a href="https://arxiv.org/abs/2302.06833">arXiv</a>
        /
        <p></p>
        <p>ViT-VQGAN plus a NeRF-based decoder that enables both single-image view synthesis and 3D generation.</p>
      </td>
    </tr>

    <tr onmouseout="personnerf_stop()" onmouseover="personnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='personnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/personnerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/personnerf.jpg' width="160">
        </div>
        <script type="text/javascript">
          function personnerf_start() {
            document.getElementById('personnerf_image').style.opacity = "1";
          }

          function personnerf_stop() {
            document.getElementById('personnerf_image').style.opacity = "0";
          }
          personnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://grail.cs.washington.edu/projects/personnerf/">
          <span class="papertitle">PersonNeRF: Personalized Reconstruction from Photo Collections</span>
        </a>
        <br>
        <a href="https://chungyiweng.github.io/">Chung-Yi Weng</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
        <a href="https://homes.cs.washington.edu/~kemelmi/">Ira Kemelmacher-Shlizerman</a>
        <br>
  <em>CVPR</em>, 2023
        <br>
        <a href="https://grail.cs.washington.edu/projects/personnerf/">project page</a>
        /
        <a href="https://arxiv.org/abs/2302.08504">arXiv</a>
        /
        <a href="https://youtu.be/2k_5hRSoon4">video</a>
        <p></p>
        <p>Construct a personalized 3D model from an unstructed photo collection.</p>
      </td>
    </tr>

    <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bhnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/bhnerf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/bhnerf_before.png' width="160">
        </div>
        <script type="text/javascript">
          function bhnerf_start() {
            document.getElementById('bhnerf_image').style.opacity = "1";
          }

          function bhnerf_stop() {
            document.getElementById('bhnerf_image').style.opacity = "0";
          }
          bhnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://imaging.cms.caltech.edu/bhnerf/">
          <span class="papertitle">Gravitationally Lensed Black Hole Emission Tomography</span>
        </a>
        <br>
        <a href="https://www.aviadlevis.info/">Aviad Levis*</a>,
        <strong>Pratul Srinivasan*</strong>,
        <a href="https://achael.github.io/">Andrew A. Chael</a>,
        <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
        <a href="http://users.cms.caltech.edu/~klbouman/">Katherine L. Bouman</a>
        <br>
  <em>CVPR</em>, 2022
        <br>
        <a href="http://imaging.cms.caltech.edu/bhnerf/">project page</a>
        /
        <a href="https://arxiv.org/abs/2204.03715">arXiv</a>
        /
        <a href="https://www.youtube.com/watch?v=eFPmShxhtg0">video</a>
        <p></p>
        <p>We apply ideas from NeRF to the problem of reconstructing the dynamic emissive volume around a black hole.</p>
      </td>
    </tr>

    <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
              <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/refnerf.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/refnerf.jpeg' width="160">
        </div>
        <script type="text/javascript">
          function refnerf_start() {
            document.getElementById('refnerf_image').style.opacity = "1";
          }

          function refnerf_stop() {
            document.getElementById('refnerf_image').style.opacity = "0";
          }
          refnerf_stop()
        </script>
      </td>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="https://dorverbin.github.io/refnerf/index.html">
            <span class="papertitle">Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</span>
          </a>
          <br>
          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://www.eecs.harvard.edu/~zickler/Main/HomePage">Todd Zickler</a>,
          <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
          <strong>Pratul Srinivasan</strong>
          <br>
    <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>
          <br>
          <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
          /
          <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
          /
          <a href="https://youtu.be/qrdRH9irAlk">video</a>
          <p></p>
          <p>We fix NeRF's shortcomings when representing shiny materials, greatly improve NeRF's normal vectors, and enable intuitive material editing.</p>
        </td>
      </tr>

    <tr onmouseout="blocknerf_stop()" onmouseover="blocknerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='blocknerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/grace_trim_square_320_crf25.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/grace_009.jpeg' width="160">
        </div>
        <script type="text/javascript">
          function blocknerf_start() {
            document.getElementById('blocknerf_image').style.opacity = "1";
          }

          function blocknerf_stop() {
            document.getElementById('blocknerf_image').style.opacity = "0";
          }
          blocknerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://waymo.com/research/block-nerf/">
          <span class="papertitle">Block-NeRF: Scalable Large Scene Neural View Synthesis</span>
        </a>
        <br>
        <a href="http://matthewtancik.com/">Matthew Tancik</a>,
        <a href="http://casser.io/">Vincent Casser</a>,
        <a href="https://sites.google.com/site/skywalkeryxc/">Xinchen Yan</a>,
        <a href="https://scholar.google.com/citations?user=5mJUkI4AAAAJ&hl=en">Sabeek Pradhan</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://www.henrikkretzschmar.com/">Henrik Kretzschmar</a>
        <br>
  <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://waymo.com/research/block-nerf/">project page</a>
        /
        <a href="https://arxiv.org/abs/2202.05263">arXiv</a>
        /
        <a href="https://www.youtube.com/watch?v=6lGMCAzBzOQ">video</a>
        <p></p>
        <p>We build city-scale scenes from many NeRFs, trained using millions of images.</p>
      </td>
    </tr>

    <tr onmouseout="hnerf_stop()" onmouseover="hnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='hnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/hnerf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/hnerf_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function hnerf_start() {
            document.getElementById('hnerf_image').style.opacity = "1";
          }

          function hnerf_stop() {
            document.getElementById('hnerf_image').style.opacity = "0";
          }
          hnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://grail.cs.washington.edu/projects/humannerf/">
          <span class="papertitle">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</span>
        </a>
        <br>
        <a href="https://homes.cs.washington.edu/~chungyi/">Chung-Yi Weng</a>,
        <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman </a>
        <br>
        <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://grail.cs.washington.edu/projects/humannerf/">project page</a>
        /
        <a href="https://arxiv.org/abs/2201.04127">arXiv</a>
        /
        <a href="https://youtu.be/GM-RoZEymmw">video</a>
        <p></p>
        <p>Free-viewpoint rendering of any body pose from a monocular video of a human.</p>
      </td>
    </tr>

        <tr onmouseout="rawnerf_stop()" onmouseover="rawnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rawnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/candle_crop_vid_fasterpull_240.mp4" type="video/mp4">
                </video></div>
                <img src='img/candle_image0_patch_darker.png' width="160">
              </div>
              <script type="text/javascript">
                function rawnerf_start() {
                  document.getElementById('rawnerf_image').style.opacity = "1";
                }

                function rawnerf_stop() {
                  document.getElementById('rawnerf_image').style.opacity = "0";
                }
                rawnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://bmild.github.io/rawnerf/index.html">
                <span class="papertitle">NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images</span>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
        <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://bmild.github.io/rawnerf/index.html">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.13679">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=JtBS4KBcKVc">video</a>
              <p></p>
              <p>We train NeRFs directly on linear raw camera images, enabling new HDR view synthesis applications and greatly increasing robustness to camera noise.</p>
            </td>
          </tr>

          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/kitchenlego_square320_crf23.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/kitchenlego_square320.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf360">
                <span class="papertitle">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</span>
              </a>
              <br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://phogzone.com/">Peter Hedman</a>
              <br>
              <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf360">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
              /
              <a href="https://youtu.be/YStDS2-Ln1s">video</a>
              <p></p>
              <p>We extend mip-NeRF to produce photorealistic results on unbounded scenes.</p>
            </td>
          </tr>

          <tr onmouseout="urf_stop()" onmouseover="urf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='urf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/urf.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/urf.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function urf_start() {
                  document.getElementById('urf_image').style.opacity = "1";
                }

                function urf_stop() {
                  document.getElementById('urf_image').style.opacity = "0";
                }
                urf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://urban-radiance-fields.github.io/">
                <span class="papertitle">Urban Radiance Fields</span>
              </a>
              <br>
							<a href="http://www.krematas.com/">Konstantinos Rematas</a>,
							<a href="https://andrewhliu.github.io/">Andrew Liu</a>,
							<strong>Pratul Srinivasan</strong>,
							<a href="https://jonbarron.info/">Jonathan T. Barron</a>,
							<a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
							<a href="https://www.cs.princeton.edu/~funk/">Tom Funkhouser</a>,
							<a href="https://sites.google.com/corp/view/vittoferrari"> Vittorio Ferrari</a>
              <br>
							<em>CVPR</em>, 2022
              <br>
              <a href="https://urban-radiance-fields.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.14643">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=qGlq5DZT6uc">video</a>
              <p></p>
              <p>
								We incorporate lidar data and explicitly model the sky to reconstruct urban environments with NeRF.</p>
            </td>
          </tr>


          <tr onmouseout="ddp_stop()" onmouseover="ddp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ddp_image'>
                  <img src='img/ddp_after.jpeg' width="160"></div>
                <img src='img/ddp_before.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function ddp_start() {
                  document.getElementById('ddp_image').style.opacity = "1";
                }

                function ddp_stop() {
                  document.getElementById('ddp_image').style.opacity = "0";
                }
                ddp_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://barbararoessle.github.io/dense_depth_priors_nerf/">
                <span class="papertitle">Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</span>
              </a>
              <br>
              <a href="https://niessnerlab.org/members/barbara_roessle/profile.html">Barbara Roessle</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://www.niessnerlab.org/">Matthias Nie√üner</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://barbararoessle.github.io/dense_depth_priors_nerf/">project page</a>
              /
              <a href="https://arxiv.org/abs/2112.03288">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=zzkvvdcvksc">video</a>
              <p></p>
              <p>
              We apply dense depth completion techniques to freely-available sparse stereo data to guide NeRF reconstructions from few input images.
              </p>
            </td>
          </tr>

          <tr onmouseout="nerfactor_stop()" onmouseover="nerfactor_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfactor_image'><img src='img/nerfactor.gif' width="160"></div>
                <img src='img/nerfactor.png' width="160">
              </div>
              <script type="text/javascript">
                function nerfactor_start() {
                  document.getElementById('nerfactor_image').style.opacity = "1";
                }

                function nerfactor_stop() {
                  document.getElementById('nerfactor_image').style.opacity = "0";
                }
                nerfactor_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">
                <span class="papertitle">NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination</span>
              </a>
              <br>
              <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://boyangdeng.com/">Boyang Deng</a>,
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
              <a href="https://billf.mit.edu/">William T. Freeman</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
        <em>ACM Transactions on Graphics (SIGGRAPH Asia)</em>, 2021
              <br>
              <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">project page</a> /
              <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a> /
              <a href="https://arxiv.org/abs/2106.01970">arXiv</a>
              <p></p>
              <p>We recover relightable NeRF-like models from images under a single unknown lighting condition.</p>
            </td>
          </tr>

          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/mipnerf_ipe.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/mipnerf_ipe.png' width="160">
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }

                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf">
                <span class="papertitle">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</span>
              </a>
              <br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <strong> Pratul Srinivasan </strong>
              <br>
        <em>ICCV</em>, 2021 <font color="tomato"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf">project page</a> /
              <a href="https://arxiv.org/abs/2103.13415">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=EpH175PY1A0">video</a>
              <p></p>
              <p>We modify NeRF to output volume density and emitted radiance at a volume of space instead of a single point to fix NeRF's issues with sampling and aliasing.</p>
            </td>
          </tr>


          <tr onmouseout="snerg_stop()" onmouseover="snerg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='snerg_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/bakenerf.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/bakenerf160.png' width="160">
              </div>
              <script type="text/javascript">
                function snerg_start() {
                  document.getElementById('snerg_image').style.opacity = "1";
                }

                function snerg_stop() {
                  document.getElementById('snerg_image').style.opacity = "0";
                }
                snerg_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="http://nerf.live">
                <span class="papertitle">Baking Neural Radiance Fields for Real-Time View Synthesis</span>
              </a>
              <br>
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <strong> Pratul Srinivasan </strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>
              <br>
        <em>ICCV</em>, 2021 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://nerf.live">project page</a> /
              <a href="https://arxiv.org/abs/2103.14645">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">video</a> /
              <a href="https://nerf.live/#demos">demo</a>
              <p></p>
              <p>We "bake" a trained NeRF into a sparse voxel grid of colors and features in order to render it in real-time.</p>
            </td>
          </tr>

          <tr onmouseout="dpdeblur_stop()" onmouseover="dpdeblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dpdeblur_image'><img src='img/dualdefocus_after.jpeg' width="160"></div>
                <img src='img/dualdefocus_before.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function dpdeblur_start() {
                  document.getElementById('dpdeblur_image').style.opacity = "1";
                }

                function dpdeblur_stop() {
                  document.getElementById('dpdeblur_image').style.opacity = "0";
                }
                dpdeblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://imaging.cs.cmu.edu/dual_pixels/">
                <span class="papertitle">Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image</span>
              </a>
              <br>
              <a href="https://shumianxin.github.io/">Shumian Xin</a>,
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
							<a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>
              <br>
        <em>ICCV</em>, 2021 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://imaging.cs.cmu.edu/dual_pixels/">project page</a> /
              <a href="https://github.com/cmu-ci-lab/dual_pixel_defocus_estimation_deblurring">code</a> /
              <a href="https://arxiv.org/abs/2110.05655">arXiv</a>
              <p></p>
              <p>We deblur dual-pixel images by representing the scene as a multiplane image and carefully considering dual-pixel optics in an optimization framework.</p>
            </td>
          </tr>


          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerv_image'><img src='img/hotdog.gif' width="160"></div>
                <img src='img/hotdog.png' width="160">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('nerv_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('nerv_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://pratulsrinivasan.github.io/nerv/">
                <span class="papertitle">NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis</span>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://boyangdeng.com/">Boyang Deng</a>,
              <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              </br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
        <em>CVPR</em>, 2021
              <br>
              <a href="https://pratulsrinivasan.github.io/nerv/">project page</a> /
              <a href="https://www.youtube.com/watch?v=4XyDdvhhjVo">video</a> /
              <a href="https://arxiv.org/abs/2012.03927">arXiv</a>
              <p></p>
              <p>We recover relightable NeRF-like models using neural approximations of expensive visibility integrals, so we can simulate complex volumetric light transport during training.</p>
            </td>
          </tr>

          <tr onmouseout="winr_stop()" onmouseover="winr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='winr_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/notre_160.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/notre.jpg' width="160">
              </div>
              <script type="text/javascript">
                function winr_start() {
                  document.getElementById('winr_image').style.opacity = "1";
                }

                function winr_stop() {
                  document.getElementById('winr_image').style.opacity = "0";
                }
                winr_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="http://www.matthewtancik.com/learnit">
                <span class="papertitle">Learned Initializations for Optimizing Coordinate-Based Neural Representations</span>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://www.linkedin.com/in/terrance-wang/">Terrance Wang</a>,
              <a href="https://www.linkedin.com/in/divi-schmidt-262044180/">Divi Schmidt</a>,
              <strong>Pratul Srinivasan</strong>,
              </br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>CVPR</em>, 2021 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://www.matthewtancik.com/learnit">project page</a> /
              <a href="https://www.youtube.com/watch?v=A-r9itCzcyo">video</a> /
              <a href="https://arxiv.org/abs/2012.02189">arXiv</a>
              <p></p>
              <p>We use meta-learning to find weight initializations for coordinate-based MLPs that allow them to converge faster and generalize better.</p>
            </td>
          </tr>

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ibrnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/ibrnet_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/ibrnet_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('ibrnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('ibrnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ibrnet.github.io/">
                <span class="papertitle">IBRNet: Learning Multi-View Image-Based Rendering</span>
              </a>
              <br>
              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
              <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
              <a href="https://www.kylegenova.com/">Kyle Genova</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>,
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://ibrnet.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2102.13090">arXiv</a>
              <p></p>
              <p>Training a network that blends source views using a NeRF-like continuous neural volumetric representation, for NeRF-like performance without per-scene training.</p>
            </td>
          </tr>

          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'><img src='img/lion_ff.jpg' width="160"></div>
                <img src='img/lion_none.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ff_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function ff_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                ff_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://bmild.github.io//fourfeat/index.html">
                <span class="papertitle">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</span>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>,
              <a href="https://www.linkedin.com/in/nithinraghavan">Nithin Raghavan</a>,
              <a href="https://scholar.google.com/citations?user=lvA86MYAAAAJ&hl=en">Utkarsh Singhal</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>NeurIPS</em>, 2020 <font color="tomato"><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a href="https://bmild.github.io//fourfeat/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2006.10739">arXiv</a> /
              <a href="https://github.com/tancik/fourier-feature-networks">code</a>
              <p></p>
              <p>Mapping input coordinates with simple Fourier features before passing them to a fully-connected network enables the network to learn much higher-frequency functions.</p>
            </td>
          </tr>

          <tr onmouseout="nrf_stop()" onmouseover="nrf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nrf_image'><video  width="160" muted autoplay loop>
                <source src="img/neural_reflectance.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/neural_reflectance.png' width="160">
              </div>
              <script type="text/javascript">
                function nrf_start() {
                  document.getElementById('nrf_image').style.opacity = "1";
                }

                function nrf_stop() {
                  document.getElementById('nrf_image').style.opacity = "0";
                }
                nrf_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2008.03824">
                <span class="papertitle">Neural Reflectance Fields for Appearance Acquisition</span>
              </a>
              <br>
              <a href="http://cseweb.ucsd.edu/~bisai/">Sai Bi*</a>,
              <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu*</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
              <a href="http://www.miloshasan.net/">Milos Hasan</a>,
              <a href="http://yannickhold.com/">Yannick Hold-Geoffroy</a>,
              <a href="https://cseweb.ucsd.edu/~kriegman/">David Kriegman</a>,
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
        <em>arXiv</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2008.03824">arXiv</a>
              <p></p>
              <p>We recover relightable NeRF-like models by predicting per-location BRDFs and surface normals, and marching light rays through the NeRF volume to compute visibility.</p>
            </td>
          </tr>

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerf_image'><img src='img/vase_small.gif' width="160"></div>
                <img src='img/vase_still.png' width="160">
              </div>
              <script type="text/javascript">
                function nerf_start() {
                  document.getElementById('nerf_image').style.opacity = "1";
                }

                function nerf_stop() {
                  document.getElementById('nerf_image').style.opacity = "0";
                }
                nerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="http://www.matthewtancik.com/nerf">
                <span class="papertitle">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</span>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>European Conference on Computer Vision (ECCV)</em>, 2020 <font color="tomato"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
              <br>
              <a href="http://www.matthewtancik.com/nerf">project page</a> /
              <a href="https://arxiv.org/abs/2003.08934">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=JuH79E8rdKc">video</a> /
              <a href="https://www.youtube.com/watch?v=LRAqeM8EjOo">technical overview</a> /
              <a href="https://github.com/bmild/nerf">code</a> /
	            <a href="https://www.youtube.com/watch?v=nCpGStnayHk">two minute papers</a>
              <p></p>
              <p>We optimize a simple neural network to represent a scene as a 5D function (3D volume + 2D view direction) from just a set of images, and synthesize photorealistic novel views.</p>
            </td>
          </tr>

          <tr onmouseout="mdp_stop()" onmouseover="mdp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='mdp_image'><video  width="160" muted autoplay loop>
              <source src="img/mdp.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
                <img src='img/mdp.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mdp_start() {
                  document.getElementById('mdp_image').style.opacity = "1";
                }

                function mdp_stop() {
                  document.getElementById('mdp_image').style.opacity = "0";
                }
                mdp_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2008.01815">
                <span class="papertitle">Deep Multi Depth Panoramas for View Synthesis</span>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/kaienlin2576/">Kai-En Lin</a>,
              <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>,
              <a href="https://pratulsrinivasan.github.io/">Ben Mildenhall</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="http://yannickhold.com/">Yannick Hold-Geoffroy</a>,
            </br>
              <a href="http://www.stephendiverdi.com/">Stephen DiVerdi</a>,
              <a href="https://qisun.me/">Qi Sun</a>,
              <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
        <em>European Conference on Computer Vision (ECCV)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2008.01815">arXiv</a> /
              <a href="https://cseweb.ucsd.edu/~zex014/papers/2020_mdp/2020_mdp.mp4">video</a>
              <p></p>
              <p>We represent scenes as multi-layer panoramas with depth for VR view synthesis.</p>
            </td>
          </tr>

          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lh_image'><img src='img/rings_crop.gif' width="160"></div>
                <img src='img/rings.png' width="160">
              </div>
              <script type="text/javascript">
                function lighthouse_start() {
                  document.getElementById('lh_image').style.opacity = "1";
                }

                function lighthouse_stop() {
                  document.getElementById('lh_image').style.opacity = "0";
                }
                lighthouse_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://pratulsrinivasan.github.io/lighthouse/">
                <span class="papertitle">Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination</span>
              </a>
              <br>
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="https://research.google/people/RichardTucker/">Richard Tucker</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
              <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2020
              <br>
              <a href="https://pratulsrinivasan.github.io/lighthouse/">project page</a> /
              <a href="https://arxiv.org/abs/2003.08367">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=KsiZpUFPqIU">video</a> /
              <a href="https://github.com/pratulsrinivasan/lighthouse">code</a>
              <p></p>
              <p>We predict a multiscale light volume from an input stereo pair, and render this volume to compute illumination at any 3D point for relighting inserted virtual objects.</p>
            </td>
          </tr>

          <tr onmouseout="llff_stop()" onmouseover="llff_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='llff_image'><img src='img/fern160.gif' width="160"></div>
                <img src='img/fern.jpg' width="160">
              </div>
              <script type="text/javascript">
                function llff_start() {
                  document.getElementById('llff_image').style.opacity = "1";
                }

                function llff_stop() {
                  document.getElementById('llff_image').style.opacity = "0";
                }
                llff_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://bmild.github.io/llff/">
                <span class="papertitle">Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</span>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://scholar.google.com/citations?user=yZMAlU4AAAAJ">Rodrigo Ortiz-Cayon</a>,
              <a href="http://faculty.cs.tamu.edu/nimak/">Nima Khademi Kalantari</a>,
            </br>
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://abhishekkar.info/">Abhishek Kar</a>
              <br>
        <em>SIGGRAPH</em>, 2019
              <br>
              <a href="https://bmild.github.io/llff/">project page</a> /
              <a href="https://arxiv.org/abs/1905.00889">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=LY6MgDUzS3M">video</a> /
              <a href="https://github.com/Fyusion/LLFF">code</a>
              <p></p>
              <p>We develop a deep learning method for rendering novel views of complex real world scenes from a small number of images, and analyze it with light field sampling theory.</p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='img/mpi_after.jpg' width="160"></div>
                <img src='img/mpi_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mpi_start() {
                  document.getElementById('mpi_image').style.opacity = "1";
                }

                function mpi_stop() {
                  document.getElementById('mpi_image').style.opacity = "0";
                }
                mpi_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1905.00413">
                <span class="papertitle">Pushing the Boundaries of View Extrapolation with Multiplane Images</span>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>, <a href="https://research.google/people/RichardTucker/">Richard Tucker</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019 &nbsp <font color="tomato"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1905.00413">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=aJqAaMNL2m4">video</a> /
              <a href="https://github.com/google-research/google-research/tree/master/mpi_extrapolation">code</a>
              <p></p>
              <p>We use Fourier theory to show the limits of view extrapolation with multiplane images, and develop a deep learning pipeline with 3D inpainting for better view extrapolation results.</p>
            </td>
          </tr>

          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='aperture_image'><img src='img/aperture_after.jpg'></div>
                <img src='img/aperture_before.jpg'>
              </div>
              <script type="text/javascript">
                function aperture_start() {
                  document.getElementById('aperture_image').style.opacity = "1";
                }

                function aperture_stop() {
                  document.getElementById('aperture_image').style.opacity = "0";
                }
                aperture_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1711.07933">
                <span class="papertitle">Aperture Supervision for Monocular Depth Estimation</span>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1711.07933">arXiv</a> /
              <a href="https://github.com/google/aperture_supervision">code</a>
              <p></p>
              <p>We train a neural network to estimate a depth map from a single image using only images with different-sized apertures as supervision, and use this to synthesize artificial bokeh.</p>
            </td>
          </tr>

          <tr onmouseout="cb_stop()" onmouseover="cb_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br><br><br><br>
              <div class="one">
                <div class="two" id='cb_image'><img src='img/chromablur_after.jpg' width="160"></div>
                <img src='img/chromablur_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function cb_start() {
                  document.getElementById('cb_image').style.opacity = "1";
                }

                function cb_stop() {
                  document.getElementById('cb_image').style.opacity = "0";
                }
                cb_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="http://bankslab.berkeley.edu/publications/chromablur/">
                <span class="papertitle">ChromaBlur: Rendering Chromatic Eye Aberration Improves Accommodation and Realism</span>
              </a>
              <br>
              <a href="https://steven.cholewiak.com/">Steven A. Cholewiak</a>,
              <a href="https://www.dur.ac.uk/physics/staff/profiles/?id=246">Gordon D. Love</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://bankslab.berkeley.edu/">Martin S. Banks</a>
              <br>
              <em>SIGGRAPH Asia</em>, 2017 &nbsp <font color="tomato"></font>
              <br>
              <a href="http://bankslab.berkeley.edu/publications/chromablur/">project page</a> /
              <a href="https://www.youtube.com/watch?v=oGZgEmkmvvg&feature=youtu.be">video</a>
              <p></p>
              <p>We show that properly considering the eye's aberrations when rendering for VR displays increases perceived realism and helps drive accomodation.</p>
            </td>
          </tr>

          <tr onmouseout="lf_stop()" onmouseover="lf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br><br>
              <div class="one">
                <div class="two" id='lf_image'><img src='img/lfsyn_after.gif' width="160"></div>
                <img src='img/lfsyn_before.gif' width="160">
              </div>
              <script type="text/javascript">
                function lf_start() {
                  document.getElementById('lf_image').style.opacity = "1";
                }

                function lf_stop() {
                  document.getElementById('lf_image').style.opacity = "0";
                }
                lf_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1708.03292">
                <span class="papertitle">Learning to Synthesize a 4D RGBD Light Field from a Single Image</span>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>, <a href="https://ssnl.github.io/">Tongzhou Wang</a>,
              Ashwin Sreelal,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2017 &nbsp <font color="tomato"><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1708.03292">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=yLCvWoQLnms&feature=youtu.be">video</a> /
              <a href="https://github.com/pratulsrinivasan/Local_Light_Field_Synthesis">code</a> /
              <a href="pdf/ICCV17_LF_Synthesis_Supplementary.pdf">supplementary PDF</a>
              <p></p>
              <p>We train a neural network to predict ray depths and RGB colors for a local light field around a single input image.</p>
            </td>
          </tr>

          <tr onmouseout="deblur_stop()" onmouseover="deblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='deblur_image'><img src='img/deblur_ours.gif' width="160"></div>
                <img src='img/deblur_blur.gif' width="160">
              </div>
              <script type="text/javascript">
                function deblur_start() {
                  document.getElementById('deblur_image').style.opacity = "1";
                }

                function deblur_stop() {
                  document.getElementById('deblur_image').style.opacity = "0";
                }
                deblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1704.05416">
                <span class="papertitle">Light Field Blind Motion Deblurring</span>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>Conference Computer Vision and Pattern Recognition (CVPR)</em>, 2017 &nbsp <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1704.05416">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=rtukre-ErmI&feature=youtu.be">video</a> /
              <a href="https://github.com/pratulsrinivasan/Light_Field_Blind_Motion_Deblurring">code</a> /
              <a href="https://pratulsrinivasan.github.io/deblur_html/supplementary.html">additional results</a>
              <p></p>
              <p>We develop Fourier theory to describe the effects of camera motion on light fields, and an optimization algorithm for deblurring light fields captured with unknown camera motion.</p>
            </td>
          </tr>

          <tr onmouseout="sf_stop()" onmouseover="sf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br>
              <div class="one">
                <div class="two" id='sf_image'><img src='img/ow_after.gif' width="160"></div>
                <img src='img/ow_before.gif' width="160">
              </div>
              <script type="text/javascript">
                function sf_start() {
                  document.getElementById('sf_image').style.opacity = "1";
                }

                function sf_stop() {
                  document.getElementById('sf_image').style.opacity = "0";
                }
                sf_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="pdf/ICCV15_LF_ORIENTED_WINDOWS.pdf">
                <span class="papertitle">Oriented Light-Field Windows for Scene Flow</span>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://scholar.google.com/citations?user=P_GSjQMAAAAJ&hl=en">Michael W. Tao</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2015
              <br>
              <a href="pdf/ICCV15_LF_ORIENTED_WINDOWS.pdf">paper PDF</a> /
              <a href="code/ICCV15_LF_SCENE_FLOW.zip">code</a> /
              <a href="https://youtu.be/hENxM4lBVXo">video</a>
              <p></p>
              <p>We develop a 4D light field descriptor and an algorithm to use these to compute scene flow (3D motion of observed points) from two captured light fields.</p>
            </td>
          </tr>

          <tr onmouseout="lfd_stop()" onmouseover="lfd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lfd_image'><img src='img/lfd_after.png' width="160"></div>
                <img src='img/lfd_before.png' width="160">
              </div>
              <script type="text/javascript">
                function lfd_start() {
                  document.getElementById('lfd_image').style.opacity = "1";
                }

                function lfd_stop() {
                  document.getElementById('lfd_image').style.opacity = "0";
                }
                lfd_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tao_Depth_From_Shading_2015_CVPR_paper.pdf">
                <span class="papertitle">Shape Estimation from Shading, Defocus, and Correspondence Using Light-Field Angular Coherence</span>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=P_GSjQMAAAAJ&hl=en">Michael W. Tao</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://scholar.google.com/citations?user=4g-njrYAAAAJ&hl=en">Sunil Hadap</a>,
              <a href="https://www.cs.princeton.edu/~smr/">Szymon Rusinkiewicz</a>,
            </br>
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>IEEE Transactions on Pattern Matching and Machine Intelligence (PAMI)</em>, 2017 and <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015
              <br>
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tao_Depth_From_Shading_2015_CVPR_paper.pdf">conference PDF</a> /
              <a href="https://cseweb.ucsd.edu/~ravir/normals_PAMI.pdf">journal PDF</a> /
              <a href="code/CVPR15_LF_DEPTH_SHADING.zip">code</a>
              <p></p>
              <p>We develop an algorithm that jointly considers cues from defocus, correspondence, and shading to estimate better depths from a light field.</p>
            </td>
          </tr>

          <tr onmouseout="dd_stop()" onmouseover="dd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br><br>
              <div class="one">
                <div class="two" id='dd_image'><img src='img/dd_after.png' width="160"></div>
                <img src='img/dd_before.png' width="160">
              </div>
              <script type="text/javascript">
                function dd_start() {
                  document.getElementById('dd_image').style.opacity = "1";
                }

                function dd_stop() {
                  document.getElementById('dd_image').style.opacity = "0";
                }
                dd_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-10-3568">
                <span class="papertitle">Fully Automated Detection of Diabetic Macular Edema and Dry Age-Related Macular Degeneration from Optical Coherence Tomography Images</span>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              Leo A. Kim, Priyatham S. Mettu, Scott W. Cousins, Grant M. Comer,
              <a href="https://bme.duke.edu/faculty/joseph-izatt">Joseph A. Izatt</a>,
              <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a>
              <br>
              <em>Biomedical Optics Express</em>, 2014
              <br>
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-10-3568">journal article</a> /
              <a href="http://people.duke.edu/~sf59/Srinivasan_BOE_2014_dataset.htm">dataset</a>
              <p></p>
              <p>We develop a classification algorithm to detect diseases from OCT images of the retina.</p>
            </td>
          </tr>

          <tr onmouseout="seg_stop()" onmouseover="seg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br>
              <div class="one">
                <div class="two" id='seg_image'><img src='img/seg_after.png'></div>
                <img src='img/seg_before.png'>
              </div>
              <script type="text/javascript">
                function seg_start() {
                  document.getElementById('seg_image').style.opacity = "1";
                }

                function seg_stop() {
                  document.getElementById('seg_image').style.opacity = "0";
                }
                seg_stop()
              </script>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1704.05416">
                <span class="papertitle">Automatic Segmentation of up to Ten Layer Boundaries in SD-OCT Images of the Mouse Retina With and Without Missing Layers due to Pathology</span>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              Stephanie J. Heflin,
              <a href="https://bme.duke.edu/faculty/joseph-izatt">Joseph A. Izatt</a>,
              <a href="https://dukeeyecenter.duke.edu/about/faculty/vadim-y-arshavsky-phd">Vadim Y. Arshavsky</a>,
              <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a>
              <br>
              <em>Biomedical Optics Express</em>, 2014
              <br>
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-2-348">journal article</a>
              <p></p>
              <p>We develop a segmentation algorithm to quantify the shape of retinal layers in OCT images that is robust to deformations due to disease.</p>
            </td>
          </tr>
      </tbody>
      </table>

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <h2>Teaching</h2>
        </td>
      </tr>
      </tbody>
      </table> -->

      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr onmouseout="box_stop()" onmouseover="box_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='box_image'><img src='img/cbox.gif' width="160"></div>
            <img src='img/cbox_before.gif' width="160">
          </div>
          <script type="text/javascript">
            function box_start() {
              document.getElementById('box_image').style.opacity = "1";
            }

            function box_stop() {
              document.getElementById('box_image').style.opacity = "0";
            }
            box_stop()
          </script>
        </td>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <p>
            <a href="https://cs184.eecs.berkeley.edu/sp18">
            <span class="papertitle">CS184 - Computer Graphics and Imaging, Spring 2018 (GSI)</span>
            </a>
            <br><br>
            <a href="https://cs184.eecs.berkeley.edu/sp19">
            <span class="papertitle">CS184 - Computer Graphics and Imaging, Spring 2019 (GSI)</span>
            </a>
            <br>
          </p>
        </td>
      </tr>

    </tbody>
    </table> -->

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          You've probably seen this website template before, thanks to <a href="https://jonbarron.info">Jon Barron</a>. <br>
          Last updated December 2023.
        </p>
        </td>
      </tr>
      </tbody>
      </table>

    </td>
    </tr>
  </table>
  </body>
</html>
