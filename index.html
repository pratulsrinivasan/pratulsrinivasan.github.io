
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 22px;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 32px;
}

.one {
width: 160px;
height: 160px;
position: relative;
}

.two {
width: 160px;
height: 160px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
  <link rel="icon" type="image/png" href="img/icon.png">
  <title>Pratul Srinivasan</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Pratul Srinivasan</name>
        </p>
        <p>
          I'm a research scientist at <a href="https://research.google/">Google Research</a>, where I work on problems at the intersection of computer vision, computer graphics, and machine learning.
        </p>
        <p>
          I received my PhD from the <a href="https://eecs.berkeley.edu/">EECS Department</a> at UC Berkeley in December 2020, where I was advised by <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a> and <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>. At UC Berkeley, I was a member of the <a href="https://bair.berkeley.edu/">Berkeley AI Research (BAIR)</a> lab.
        </p>
        <p>
          During my PhD, I interned twice at <a href="https://research.google/">Google Research</a>: at Mountain View in 2017 (hosted by <a href="https://jonbarron.info/">Jon Barron</a> in <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy's</a> group) and at New York City in 2018 (hosted by <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>).
        </p>
        <p>
          I graduated from Duke University in 2014, where I majored in Biomedical Engineering and Computer Science. At Duke, I worked with <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a> on research problems in medical computer vision.
        </p>
        <p>
          I grew up in Palo Alto, CA and graduated from Henry M. Gunn High School in 2010.
        </p>
        <p align=center>
          <a href="mailto:pratul.srinivasan@gmail.com">Email</a> &nbsp/&nbsp
          <a href="pdf/cv.pdf">CV</a> &nbsp/&nbsp
	        <a href="https://scholar.google.com/citations?user=aYyDsZ0AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
          <a href="https://twitter.com/_pratul_">Twitter</a>
        </p>
        </td>
        <td width="33%">
	  <a href="img/avatar.jpg">
        <img src="img/avatar_circle.png" width="250px"></a>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research and Publications</heading>
          <p>
            * denotes equal contribution co-authorship

          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bhnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/bhnerf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/bhnerf_before.png' width="160">
        </div>
        <script type="text/javascript">
          function bhnerf_start() {
            document.getElementById('bhnerf_image').style.opacity = "1";
          }

          function bhnerf_stop() {
            document.getElementById('bhnerf_image').style.opacity = "0";
          }
          bhnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://imaging.cms.caltech.edu/bhnerf/">
          <papertitle>Gravitationally Lensed Black Hole Emission Tomography</papertitle>
        </a>
        <br>
        <a href="https://www.aviadlevis.info/">Aviad Levis*</a>,
        <strong>Pratul Srinivasan*</strong>,
        <a href="https://achael.github.io/">Andrew A. Chael</a>,
        <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
        <a href="http://users.cms.caltech.edu/~klbouman/">Katherine L. Bouman</a>
        <br>
  <em>CVPR</em>, 2022
        <br>
        <a href="http://imaging.cms.caltech.edu/bhnerf/">project page</a>
  /
        <a href="https://arxiv.org/abs/2204.03715">arXiv</a>
  /
        <a href="https://www.youtube.com/watch?v=eFPmShxhtg0">video</a>
        <p></p>
        <p>We apply ideas from NeRF to the problem of reconstructing the dynamic emissive volume around a black hole.</p>
      </td>
    </tr>

    <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
      <td width="25%">
        <div class="one">
              <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/refnerf.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/refnerf.jpeg' width="160">
        </div>
        <script type="text/javascript">
          function refnerf_start() {
            document.getElementById('refnerf_image').style.opacity = "1";
          }

          function refnerf_stop() {
            document.getElementById('refnerf_image').style.opacity = "0";
          }
          refnerf_stop()
        </script>
      </td>
        <td valign="top" width="75%">
          <a href="https://dorverbin.github.io/refnerf/index.html">
            <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>
          </a>
          <br>
          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://www.eecs.harvard.edu/~zickler/Main/HomePage">Todd Zickler</a>,
          <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
          <strong>Pratul Srinivasan</strong>
          <br>
    <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>
          <br>
          <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
    /
          <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
    /
          <a href="https://youtu.be/qrdRH9irAlk">video</a>
          <p></p>
          <p>We fix NeRF's shortcomings when representing shiny materials, greatly improve NeRF's normal vectors, and enable intuitive material editing.</p>
        </td>
      </tr>

    <tr onmouseout="blocknerf_stop()" onmouseover="blocknerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='blocknerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/grace_trim_square_320_crf25.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/grace_009.jpeg' width="160">
        </div>
        <script type="text/javascript">
          function blocknerf_start() {
            document.getElementById('blocknerf_image').style.opacity = "1";
          }

          function blocknerf_stop() {
            document.getElementById('blocknerf_image').style.opacity = "0";
          }
          blocknerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://waymo.com/research/block-nerf/">
          <papertitle>Block-NeRF: Scalable Large Scene Neural View Synthesis</papertitle>
        </a>
        <br>
        <a href="http://matthewtancik.com/">Matthew Tancik</a>,
        <a href="http://casser.io/">Vincent Casser</a>,
        <a href="https://sites.google.com/site/skywalkeryxc/">Xinchen Yan</a>,
        <a href="https://scholar.google.com/citations?user=5mJUkI4AAAAJ&hl=en">Sabeek Pradhan</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://www.henrikkretzschmar.com/">Henrik Kretzschmar</a>
        <br>
  <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://waymo.com/research/block-nerf/">project page</a>
  /
        <a href="https://arxiv.org/abs/2202.05263">arXiv</a>
  /
        <a href="https://www.youtube.com/watch?v=6lGMCAzBzOQ">video</a>
        <p></p>
        <p>We build city-scale scenes from many NeRFs, trained using millions of images.</p>
      </td>
    </tr>

    <tr onmouseout="hnerf_stop()" onmouseover="hnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='hnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/hnerf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/hnerf_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function hnerf_start() {
            document.getElementById('hnerf_image').style.opacity = "1";
          }

          function hnerf_stop() {
            document.getElementById('hnerf_image').style.opacity = "0";
          }
          hnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://grail.cs.washington.edu/projects/humannerf/">
          <papertitle>HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</papertitle>
        </a>
        <br>
        <a href="https://homes.cs.washington.edu/~chungyi/">Chung-Yi Weng</a>,
        <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
        <strong>Pratul Srinivasan</strong>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman </a>
        <br>
        <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://grail.cs.washington.edu/projects/humannerf/">project page</a>
        /
        <a href="https://arxiv.org/abs/2201.04127">arXiv</a>
        /
        <a href="https://youtu.be/GM-RoZEymmw">video</a>
        <p></p>
        <p>Free-viewpoint rendering of any body pose from a monocular video of a human.</p>
      </td>
    </tr>

        <tr onmouseout="rawnerf_stop()" onmouseover="rawnerf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='rawnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/candle_crop_vid_fasterpull_240.mp4" type="video/mp4">
                </video></div>
                <img src='img/candle_image0_patch_darker.png' width="160">
              </div>
              <script type="text/javascript">
                function rawnerf_start() {
                  document.getElementById('rawnerf_image').style.opacity = "1";
                }

                function rawnerf_stop() {
                  document.getElementById('rawnerf_image').style.opacity = "0";
                }
                rawnerf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://bmild.github.io/rawnerf/index.html">
                <papertitle>NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
        <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://bmild.github.io/rawnerf/index.html">project page</a>
        /
              <a href="https://arxiv.org/abs/2111.13679">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=JtBS4KBcKVc">video</a>
              <p></p>
              <p>We train NeRFs directly on linear raw camera images, enabling new HDR view synthesis applications and greatly increasing robustness to camera noise.</p>
            </td>
          </tr>

          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/kitchenlego_square320_crf23.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/kitchenlego_square320.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf360">
                <papertitle>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://phogzone.com/">Peter Hedman</a>
              <br>
              <em>CVPR</em>, 2022 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf360">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
              /
              <a href="https://youtu.be/YStDS2-Ln1s">video</a>
              <p></p>
              <p>We extend mip-NeRF to produce photorealistic results on unbounded scenes.</p>
            </td>
          </tr>

          <tr onmouseout="urf_stop()" onmouseover="urf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='urf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/urf.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/urf.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function urf_start() {
                  document.getElementById('urf_image').style.opacity = "1";
                }

                function urf_stop() {
                  document.getElementById('urf_image').style.opacity = "0";
                }
                urf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://urban-radiance-fields.github.io/">
                <papertitle>Urban Radiance Fields</papertitle>
              </a>
              <br>
							<a href="http://www.krematas.com/">Konstantinos Rematas</a>,
							<a href="https://andrewhliu.github.io/">Andrew Liu</a>,
							<strong>Pratul Srinivasan</strong>,
							<a href="https://jonbarron.info/">Jonathan T. Barron</a>,
							<a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
							<a href="https://www.cs.princeton.edu/~funk/">Tom Funkhouser</a>,
							<a href="https://sites.google.com/corp/view/vittoferrari"> Vittorio Ferrari</a>
              <br>
							<em>CVPR</em>, 2022
              <br>
              <a href="https://urban-radiance-fields.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.14643">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=qGlq5DZT6uc">video</a>
              <p></p>
              <p>
								We incorporate lidar data and explicitly model the sky to reconstruct urban environments with NeRF.</p>
            </td>
          </tr>


          <tr onmouseout="ddp_stop()" onmouseover="ddp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ddp_image'>
                  <img src='img/ddp_after.jpeg' width="160"></div>
                <img src='img/ddp_before.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function ddp_start() {
                  document.getElementById('ddp_image').style.opacity = "1";
                }

                function ddp_stop() {
                  document.getElementById('ddp_image').style.opacity = "0";
                }
                ddp_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://barbararoessle.github.io/dense_depth_priors_nerf/">
                <papertitle>Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</papertitle>
              </a>
              <br>
              <a href="https://niessnerlab.org/members/barbara_roessle/profile.html">Barbara Roessle</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://www.niessnerlab.org/">Matthias Nießner</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://barbararoessle.github.io/dense_depth_priors_nerf/">project page</a>
              /
              <a href="https://arxiv.org/abs/2112.03288">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=zzkvvdcvksc">video</a>
              <p></p>
              <p>
              We apply dense depth completion techniques to freely-available sparse stereo data to guide NeRF reconstructions from few input images.
              </p>
            </td>
          </tr>

          <tr onmouseout="nerfactor_stop()" onmouseover="nerfactor_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='nerfactor_image'><img src='img/nerfactor.gif' width="160"></div>
                <img src='img/nerfactor.png' width="160">
              </div>
              <script type="text/javascript">
                function nerfactor_start() {
                  document.getElementById('nerfactor_image').style.opacity = "1";
                }

                function nerfactor_stop() {
                  document.getElementById('nerfactor_image').style.opacity = "0";
                }
                nerfactor_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">
                <papertitle>NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination</papertitle>
              </a>
              <br>
              <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://boyangdeng.com/">Boyang Deng</a>,
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
              <a href="https://billf.mit.edu/">William T. Freeman</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
        <em>ACM Transactions on Graphics (SIGGRAPH Asia)</em>, 2021
              <br>
              <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">project page</a> /
              <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a> /
              <a href="https://arxiv.org/abs/2106.01970">arXiv</a>
              <p></p>
              <p>We recover relightable NeRF-like models from images under a single unknown lighting condition.</p>
            </td>
          </tr>

          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/mipnerf_ipe.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/mipnerf_ipe.png' width="160">
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }

                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://jonbarron.info/mipnerf">
                <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <strong> Pratul Srinivasan </strong>
              <br>
        <em>ICCV</em>, 2021 <font color="tomato"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf">project page</a> /
              <a href="https://arxiv.org/abs/2103.13415">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=EpH175PY1A0">video</a>
              <p></p>
              <p>We modify NeRF to output volume density and emitted radiance at a volume of space instead of a single point to fix NeRF's issues with sampling and aliasing.</p>
            </td>
          </tr>


          <tr onmouseout="snerg_stop()" onmouseover="snerg_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='snerg_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/bakenerf.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/bakenerf160.png' width="160">
              </div>
              <script type="text/javascript">
                function snerg_start() {
                  document.getElementById('snerg_image').style.opacity = "1";
                }

                function snerg_stop() {
                  document.getElementById('snerg_image').style.opacity = "0";
                }
                snerg_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://nerf.live">
                <papertitle>Baking Neural Radiance Fields for Real-Time View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <strong> Pratul Srinivasan </strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>
              <br>
        <em>ICCV</em>, 2021 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://nerf.live">project page</a> /
              <a href="https://arxiv.org/abs/2103.14645">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">video</a> /
              <a href="https://nerf.live/#demos">demo</a>
              <p></p>
              <p>We "bake" a trained NeRF into a sparse voxel grid of colors and features in order to render it in real-time.</p>
            </td>
          </tr>

          <tr onmouseout="dpdeblur_stop()" onmouseover="dpdeblur_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='dpdeblur_image'><img src='img/dualdefocus_after.jpeg' width="160"></div>
                <img src='img/dualdefocus_before.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function dpdeblur_start() {
                  document.getElementById('dpdeblur_image').style.opacity = "1";
                }

                function dpdeblur_stop() {
                  document.getElementById('dpdeblur_image').style.opacity = "0";
                }
                dpdeblur_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://imaging.cs.cmu.edu/dual_pixels/">
                <papertitle>Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image</papertitle>
              </a>
              <br>
              <a href="https://shumianxin.github.io/">Shumian Xin</a>,
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
							<a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>
              <br>
        <em>ICCV</em>, 2021 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://imaging.cs.cmu.edu/dual_pixels/">project page</a> /
              <a href="https://github.com/cmu-ci-lab/dual_pixel_defocus_estimation_deblurring">code</a> /
              <a href="https://arxiv.org/abs/2110.05655">arXiv</a>
              <p></p>
              <p>We deblur dual-pixel images by representing the scene as a multiplane image and carefully considering dual-pixel optics in an optimization framework.</p>
            </td>
          </tr>


          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='nerv_image'><img src='img/hotdog.gif' width="160"></div>
                <img src='img/hotdog.png' width="160">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('nerv_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('nerv_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://pratulsrinivasan.github.io/nerv/">
                <papertitle>NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://boyangdeng.com/">Boyang Deng</a>,
              <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              </br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
        <em>CVPR</em>, 2021
              <br>
              <a href="https://pratulsrinivasan.github.io/nerv/">project page</a> /
              <a href="https://www.youtube.com/watch?v=4XyDdvhhjVo">video</a> /
              <a href="https://arxiv.org/abs/2012.03927">arXiv</a>
              <p></p>
              <p>We recover relightable NeRF-like models using neural approximations of expensive visibility integrals, so we can simulate complex volumetric light transport during training.</p>
            </td>
          </tr>

          <tr onmouseout="winr_stop()" onmouseover="winr_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='winr_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/notre_160.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/notre.jpg' width="160">
              </div>
              <script type="text/javascript">
                function winr_start() {
                  document.getElementById('winr_image').style.opacity = "1";
                }

                function winr_stop() {
                  document.getElementById('winr_image').style.opacity = "0";
                }
                winr_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://www.matthewtancik.com/learnit">
                <papertitle>Learned Initializations for Optimizing Coordinate-Based Neural Representations</papertitle>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://www.linkedin.com/in/terrance-wang/">Terrance Wang</a>,
              <a href="https://www.linkedin.com/in/divi-schmidt-262044180/">Divi Schmidt</a>,
              <strong>Pratul Srinivasan</strong>,
              </br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>CVPR</em>, 2021 <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://www.matthewtancik.com/learnit">project page</a> /
              <a href="https://www.youtube.com/watch?v=A-r9itCzcyo">video</a> /
              <a href="https://arxiv.org/abs/2012.02189">arXiv</a>
              <p></p>
              <p>We use meta-learning to find weight initializations for coordinate-based MLPs that allow them to converge faster and generalize better.</p>
            </td>
          </tr>

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ibrnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/ibrnet_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/ibrnet_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('ibrnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('ibrnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ibrnet.github.io/">
                <papertitle>IBRNet: Learning Multi-View Image-Based Rendering</papertitle>
              </a>
              <br>
              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
              <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
              <a href="https://www.kylegenova.com/">Kyle Genova</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>,
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://ibrnet.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2102.13090">arXiv</a>
              <p></p>
              <p>Training a network that blends source views using a NeRF-like continuous neural volumetric representation, for NeRF-like performance without per-scene training.</p>
            </td>
          </tr>

          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='ff_image'><img src='img/lion_ff.jpg' width="160"></div>
                <img src='img/lion_none.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ff_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function ff_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                ff_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://bmild.github.io//fourfeat/index.html">
                <papertitle>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</papertitle>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>,
              <a href="https://www.linkedin.com/in/nithinraghavan">Nithin Raghavan</a>,
              <a href="https://scholar.google.com/citations?user=lvA86MYAAAAJ&hl=en">Utkarsh Singhal</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>NeurIPS</em>, 2020 <font color="tomato"><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a href="https://bmild.github.io//fourfeat/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2006.10739">arXiv</a> /
              <a href="https://github.com/tancik/fourier-feature-networks">code</a>
              <p></p>
              <p>Mapping input coordinates with simple Fourier features before passing them to a fully-connected network enables the network to learn much higher-frequency functions.</p>
            </td>
          </tr>

          <tr onmouseout="nrf_stop()" onmouseover="nrf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='nrf_image'><video  width="160" muted autoplay loop>
                <source src="img/neural_reflectance.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/neural_reflectance.png' width="160">
              </div>
              <script type="text/javascript">
                function nrf_start() {
                  document.getElementById('nrf_image').style.opacity = "1";
                }

                function nrf_stop() {
                  document.getElementById('nrf_image').style.opacity = "0";
                }
                nrf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2008.03824">
                <papertitle>Neural Reflectance Fields for Appearance Acquisition</papertitle>
              </a>
              <br>
              <a href="http://cseweb.ucsd.edu/~bisai/">Sai Bi*</a>,
              <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu*</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
              <a href="http://www.miloshasan.net/">Milos Hasan</a>,
              <a href="http://yannickhold.com/">Yannick Hold-Geoffroy</a>,
              <a href="https://cseweb.ucsd.edu/~kriegman/">David Kriegman</a>,
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
        <em>arXiv</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2008.03824">arXiv</a>
              <p></p>
              <p>We recover relightable NeRF-like models by predicting per-location BRDFs and surface normals, and marching light rays through the NeRF volume to compute visibility.</p>
            </td>
          </tr>

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='nerf_image'><img src='img/vase_small.gif' width="160"></div>
                <img src='img/vase_still.png' width="160">
              </div>
              <script type="text/javascript">
                function nerf_start() {
                  document.getElementById('nerf_image').style.opacity = "1";
                }

                function nerf_stop() {
                  document.getElementById('nerf_image').style.opacity = "0";
                }
                nerf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://www.matthewtancik.com/nerf">
                <papertitle>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>European Conference on Computer Vision (ECCV)</em>, 2020 <font color="tomato"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
              <br>
              <a href="http://www.matthewtancik.com/nerf">project page</a> /
              <a href="https://arxiv.org/abs/2003.08934">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=JuH79E8rdKc">video</a> /
              <a href="https://www.youtube.com/watch?v=LRAqeM8EjOo">technical overview</a> /
              <a href="https://github.com/bmild/nerf">code</a> /
	            <a href="https://www.youtube.com/watch?v=nCpGStnayHk">two minute papers</a>
              <p></p>
              <p>We optimize a simple neural network to represent a scene as a 5D function (3D volume + 2D view direction) from just a set of images, and synthesize photorealistic novel views.</p>
            </td>
          </tr>

          <tr onmouseout="mdp_stop()" onmouseover="mdp_start()">
            <td width="25%">
              <div class="two" id='mdp_image'><video  width="160" muted autoplay loop>
              <source src="img/mdp.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
                <img src='img/mdp.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mdp_start() {
                  document.getElementById('mdp_image').style.opacity = "1";
                }

                function mdp_stop() {
                  document.getElementById('mdp_image').style.opacity = "0";
                }
                mdp_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2008.01815">
                <papertitle>Deep Multi Depth Panoramas for View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/kaienlin2576/">Kai-En Lin</a>,
              <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>,
              <a href="https://pratulsrinivasan.github.io/">Ben Mildenhall</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="http://yannickhold.com/">Yannick Hold-Geoffroy</a>,
            </br>
              <a href="http://www.stephendiverdi.com/">Stephen DiVerdi</a>,
              <a href="https://qisun.me/">Qi Sun</a>,
              <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
        <em>European Conference on Computer Vision (ECCV)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2008.01815">arXiv</a> /
              <a href="https://cseweb.ucsd.edu/~zex014/papers/2020_mdp/2020_mdp.mp4">video</a>
              <p></p>
              <p>We represent scenes as multi-layer panoramas with depth for VR view synthesis.</p>
            </td>
          </tr>

          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='lh_image'><img src='img/rings_crop.gif' width="160"></div>
                <img src='img/rings.png' width="160">
              </div>
              <script type="text/javascript">
                function lighthouse_start() {
                  document.getElementById('lh_image').style.opacity = "1";
                }

                function lighthouse_stop() {
                  document.getElementById('lh_image').style.opacity = "0";
                }
                lighthouse_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://pratulsrinivasan.github.io/lighthouse/">
                <papertitle>Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="https://research.google/people/RichardTucker/">Richard Tucker</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
              <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2020
              <br>
              <a href="https://pratulsrinivasan.github.io/lighthouse/">project page</a> /
              <a href="https://arxiv.org/abs/2003.08367">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=KsiZpUFPqIU">video</a> /
              <a href="https://github.com/pratulsrinivasan/lighthouse">code</a>
              <p></p>
              <p>We predict a multiscale light volume from an input stereo pair, and render this volume to compute illumination at any 3D point for relighting inserted virtual objects.</p>
            </td>
          </tr>

          <tr onmouseout="llff_stop()" onmouseover="llff_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='llff_image'><img src='img/fern160.gif' width="160"></div>
                <img src='img/fern.jpg' width="160">
              </div>
              <script type="text/javascript">
                function llff_start() {
                  document.getElementById('llff_image').style.opacity = "1";
                }

                function llff_stop() {
                  document.getElementById('llff_image').style.opacity = "0";
                }
                llff_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://bmild.github.io/llff/">
                <papertitle>Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <strong>Pratul Srinivasan*</strong>,
              <a href="https://scholar.google.com/citations?user=yZMAlU4AAAAJ">Rodrigo Ortiz-Cayon</a>,
              <a href="http://faculty.cs.tamu.edu/nimak/">Nima Khademi Kalantari</a>,
            </br>
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://abhishekkar.info/">Abhishek Kar</a>
              <br>
        <em>SIGGRAPH</em>, 2019
              <br>
              <a href="https://bmild.github.io/llff/">project page</a> /
              <a href="https://arxiv.org/abs/1905.00889">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=LY6MgDUzS3M">video</a> /
              <a href="https://github.com/Fyusion/LLFF">code</a>
              <p></p>
              <p>We develop a deep learning method for rendering novel views of complex real world scenes from a small number of images, and analyze it with light field sampling theory.</p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='mpi_image'><img src='img/mpi_after.jpg' width="160"></div>
                <img src='img/mpi_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mpi_start() {
                  document.getElementById('mpi_image').style.opacity = "1";
                }

                function mpi_stop() {
                  document.getElementById('mpi_image').style.opacity = "0";
                }
                mpi_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1905.00413">
                <papertitle>Pushing the Boundaries of View Extrapolation with Multiplane Images</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>, <a href="https://research.google/people/RichardTucker/">Richard Tucker</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
            </br>
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019 &nbsp <font color="tomato"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1905.00413">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=aJqAaMNL2m4">video</a> /
              <a href="https://github.com/google-research/google-research/tree/master/mpi_extrapolation">code</a>
              <p></p>
              <p>We use Fourier theory to show the limits of view extrapolation with multiplane images, and develop a deep learning pipeline with 3D inpainting for better view extrapolation results.</p>
            </td>
          </tr>

          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='aperture_image'><img src='img/aperture_after.jpg'></div>
                <img src='img/aperture_before.jpg'>
              </div>
              <script type="text/javascript">
                function aperture_start() {
                  document.getElementById('aperture_image').style.opacity = "1";
                }

                function aperture_stop() {
                  document.getElementById('aperture_image').style.opacity = "0";
                }
                aperture_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1711.07933">
                <papertitle>Aperture Supervision for Monocular Depth Estimation</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1711.07933">arXiv</a> /
              <a href="https://github.com/google/aperture_supervision">code</a>
              <p></p>
              <p>We train a neural network to estimate a depth map from a single image using only images with different-sized apertures as supervision, and use this to synthesize artificial bokeh.</p>
            </td>
          </tr>

          <tr onmouseout="cb_stop()" onmouseover="cb_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='cb_image'><img src='img/chromablur_after.jpg' width="160"></div>
                <img src='img/chromablur_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function cb_start() {
                  document.getElementById('cb_image').style.opacity = "1";
                }

                function cb_stop() {
                  document.getElementById('cb_image').style.opacity = "0";
                }
                cb_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://bankslab.berkeley.edu/publications/chromablur/">
                <papertitle>ChromaBlur: Rendering Chromatic Eye Aberration Improves Accommodation and Realism</papertitle>
              </a>
              <br>
              <a href="https://steven.cholewiak.com/">Steven A. Cholewiak</a>,
              <a href="https://www.dur.ac.uk/physics/staff/profiles/?id=246">Gordon D. Love</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://bankslab.berkeley.edu/">Martin S. Banks</a>
              <br>
              <em>SIGGRAPH Asia</em>, 2017 &nbsp <font color="tomato"></font>
              <br>
              <a href="http://bankslab.berkeley.edu/publications/chromablur/">project page</a> /
              <a href="https://www.youtube.com/watch?v=oGZgEmkmvvg&feature=youtu.be">video</a>
              <p></p>
              <p>We show that properly considering the eye's aberrations when rendering for VR displays increases perceived realism and helps drive accomodation.</p>
            </td>
          </tr>

          <tr onmouseout="lf_stop()" onmouseover="lf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='lf_image'><img src='img/lfsyn_after.gif' width="160"></div>
                <img src='img/lfsyn_before.gif' width="160">
              </div>
              <script type="text/javascript">
                function lf_start() {
                  document.getElementById('lf_image').style.opacity = "1";
                }

                function lf_stop() {
                  document.getElementById('lf_image').style.opacity = "0";
                }
                lf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1708.03292">
                <papertitle>Learning to Synthesize a 4D RGBD Light Field from a Single Image</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>, <a href="https://ssnl.github.io/">Tongzhou Wang</a>,
              Ashwin Sreelal,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2017 &nbsp <font color="tomato"><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1708.03292">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=yLCvWoQLnms&feature=youtu.be">video</a> /
              <a href="https://github.com/pratulsrinivasan/Local_Light_Field_Synthesis">code</a> /
              <a href="pdf/ICCV17_LF_Synthesis_Supplementary.pdf">supplementary PDF</a>
              <p></p>
              <p>We train a neural network to predict ray depths and RGB colors for a local light field around a single input image.</p>
            </td>
          </tr>

          <tr onmouseout="deblur_stop()" onmouseover="deblur_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='deblur_image'><img src='img/deblur_ours.gif' width="160"></div>
                <img src='img/deblur_blur.gif' width="160">
              </div>
              <script type="text/javascript">
                function deblur_start() {
                  document.getElementById('deblur_image').style.opacity = "1";
                }

                function deblur_stop() {
                  document.getElementById('deblur_image').style.opacity = "0";
                }
                deblur_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1704.05416">
                <papertitle>Light Field Blind Motion Deblurring</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>Conference Computer Vision and Pattern Recognition (CVPR)</em>, 2017 &nbsp <font color="tomato"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1704.05416">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=rtukre-ErmI&feature=youtu.be">video</a> /
              <a href="https://github.com/pratulsrinivasan/Light_Field_Blind_Motion_Deblurring">code</a> /
              <a href="https://pratulsrinivasan.github.io/deblur_html/supplementary.html">additional results</a>
              <p></p>
              <p>We develop Fourier theory to describe the effects of camera motion on light fields, and an optimization algorithm for deblurring light fields captured with unknown camera motion.</p>
            </td>
          </tr>

          <tr onmouseout="sf_stop()" onmouseover="sf_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='sf_image'><img src='img/ow_after.gif' width="160"></div>
                <img src='img/ow_before.gif' width="160">
              </div>
              <script type="text/javascript">
                function sf_start() {
                  document.getElementById('sf_image').style.opacity = "1";
                }

                function sf_stop() {
                  document.getElementById('sf_image').style.opacity = "0";
                }
                sf_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="pdf/ICCV15_LF_ORIENTED_WINDOWS.pdf">
                <papertitle>Oriented Light-Field Windows for Scene Flow</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              <a href="https://scholar.google.com/citations?user=P_GSjQMAAAAJ&hl=en">Michael W. Tao</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2015
              <br>
              <a href="pdf/ICCV15_LF_ORIENTED_WINDOWS.pdf">paper PDF</a> /
              <a href="code/ICCV15_LF_SCENE_FLOW.zip">code</a> /
              <a href="https://youtu.be/hENxM4lBVXo">video</a>
              <p></p>
              <p>We develop a 4D light field descriptor and an algorithm to use these to compute scene flow (3D motion of observed points) from two captured light fields.</p>
            </td>
          </tr>

          <tr onmouseout="lfd_stop()" onmouseover="lfd_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='lfd_image'><img src='img/lfd_after.png' width="160"></div>
                <img src='img/lfd_before.png' width="160">
              </div>
              <script type="text/javascript">
                function lfd_start() {
                  document.getElementById('lfd_image').style.opacity = "1";
                }

                function lfd_stop() {
                  document.getElementById('lfd_image').style.opacity = "0";
                }
                lfd_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tao_Depth_From_Shading_2015_CVPR_paper.pdf">
                <papertitle>Shape Estimation from Shading, Defocus, and Correspondence Using Light-Field Angular Coherence</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=P_GSjQMAAAAJ&hl=en">Michael W. Tao</a>,
              <strong>Pratul Srinivasan</strong>,
              <a href="https://scholar.google.com/citations?user=4g-njrYAAAAJ&hl=en">Sunil Hadap</a>,
              <a href="https://www.cs.princeton.edu/~smr/">Szymon Rusinkiewicz</a>,
            </br>
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>IEEE Transactions on Pattern Matching and Machine Intelligence (PAMI)</em>, 2017 and <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015
              <br>
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tao_Depth_From_Shading_2015_CVPR_paper.pdf">conference PDF</a> /
              <a href="https://cseweb.ucsd.edu/~ravir/normals_PAMI.pdf">journal PDF</a> /
              <a href="code/CVPR15_LF_DEPTH_SHADING.zip">code</a>
              <p></p>
              <p>We develop an algorithm that jointly considers cues from defocus, correspondence, and shading to estimate better depths from a light field.</p>
            </td>
          </tr>

          <tr onmouseout="dd_stop()" onmouseover="dd_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='dd_image'><img src='img/dd_after.png' width="160"></div>
                <img src='img/dd_before.png' width="160">
              </div>
              <script type="text/javascript">
                function dd_start() {
                  document.getElementById('dd_image').style.opacity = "1";
                }

                function dd_stop() {
                  document.getElementById('dd_image').style.opacity = "0";
                }
                dd_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-10-3568">
                <papertitle>Fully Automated Detection of Diabetic Macular Edema and Dry Age-Related Macular Degeneration from Optical Coherence Tomography Images</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              Leo A. Kim, Priyatham S. Mettu, Scott W. Cousins, Grant M. Comer,
              <a href="https://bme.duke.edu/faculty/joseph-izatt">Joseph A. Izatt</a>,
              <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a>
              <br>
              <em>Biomedical Optics Express</em>, 2014
              <br>
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-10-3568">journal article</a> /
              <a href="http://people.duke.edu/~sf59/Srinivasan_BOE_2014_dataset.htm">dataset</a>
              <p></p>
              <p>We develop a classification algorithm to detect diseases from OCT images of the retina.</p>
            </td>
          </tr>

          <tr onmouseout="seg_stop()" onmouseover="seg_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='seg_image'><img src='img/seg_after.png'></div>
                <img src='img/seg_before.png'>
              </div>
              <script type="text/javascript">
                function seg_start() {
                  document.getElementById('seg_image').style.opacity = "1";
                }

                function seg_stop() {
                  document.getElementById('seg_image').style.opacity = "0";
                }
                seg_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1704.05416">
                <papertitle>Automatic Segmentation of up to Ten Layer Boundaries in SD-OCT Images of the Mouse Retina With and Without Missing Layers due to Pathology</papertitle>
              </a>
              <br>
              <strong>Pratul Srinivasan</strong>,
              Stephanie J. Heflin,
              <a href="https://bme.duke.edu/faculty/joseph-izatt">Joseph A. Izatt</a>,
              <a href="https://dukeeyecenter.duke.edu/about/faculty/vadim-y-arshavsky-phd">Vadim Y. Arshavsky</a>,
              <a href="http://people.duke.edu/~sf59/">Sina Farsiu</a>
              <br>
              <em>Biomedical Optics Express</em>, 2014
              <br>
              <a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-5-2-348">journal article</a>
              <p></p>
              <p>We develop a segmentation algorithm to quantify the shape of retinal layers in OCT images that is robust to deformations due to disease.</p>
            </td>
          </tr>

      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Teaching</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr onmouseout="box_stop()" onmouseover="box_start()">
        <td width="25%">
          <div class="one">
            <div class="two" id='box_image'><img src='img/cbox.gif' width="160"></div>
            <img src='img/cbox_before.gif' width="160">
          </div>
          <script type="text/javascript">
            function box_start() {
              document.getElementById('box_image').style.opacity = "1";
            }

            function box_stop() {
              document.getElementById('box_image').style.opacity = "0";
            }
            box_stop()
          </script>
        </td>
        <td valign="center" width="75%">
          <p>
            <a href="https://cs184.eecs.berkeley.edu/sp18">
            <papertitle>CS184 - Computer Graphics and Imaging, Spring 2018 (GSI)</papertitle>
            </a>
            <br><br>
            <a href="https://cs184.eecs.berkeley.edu/sp19">
            <papertitle>CS184 - Computer Graphics and Imaging, Spring 2019 (GSI)</papertitle>
            </a>
            <br>
          </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          You've probably seen this website template before, thanks to <a href="https://jonbarron.info">Jon Barron</a>. <br>
          Last updated May 2020.
          <!-- http://www.cs.princeton.edu/~namana/ http://people.csail.mit.edu/janner/ http://www.sjoerdvansteenkiste.com/-->
	    </font>
        </p>
        </td>
      </tr>
      </table>

    </td>
    </tr>
  </table>
  </body>
</html>
